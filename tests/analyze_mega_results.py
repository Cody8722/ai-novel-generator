#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è¶…å¤§è¦æ¨¡æ¸¬è©¦çµæœåˆ†æç³»çµ±

åŠŸèƒ½ï¼š
1. åŠ è¼‰æ‰€æœ‰æ‰¹æ¬¡æ¸¬è©¦çµæœ
2. åˆ†æåƒæ•¸å½±éŸ¿å’Œè¶¨å‹¢
3. æ‰¾å‡ºæœ€ä½³åƒæ•¸çµ„åˆ
4. ç”Ÿæˆè©³ç´°åˆ†æå ±å‘Š
5. æä¾›å„ªåŒ–å»ºè­°

ä½¿ç”¨æ–¹æ³•ï¼š
    # ç”Ÿæˆå®Œæ•´åˆ†æå ±å‘Š
    python tests/analyze_mega_results.py

    # åªåˆ†æç‰¹å®šéšæ®µ
    python tests/analyze_mega_results.py --stage coarse
    python tests/analyze_mega_results.py --stage fine
    python tests/analyze_mega_results.py --stage validation

    # ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨ï¼ˆå¦‚æœå®‰è£äº† matplotlibï¼‰
    python tests/analyze_mega_results.py --visualize
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import json
import logging
import argparse
from datetime import datetime
from typing import Dict, List
from collections import defaultdict

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class MegaResultsAnalyzer:
    """è¶…å¤§è¦æ¨¡æ¸¬è©¦çµæœåˆ†æå™¨"""

    def __init__(self, results_dir: Path = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            results_dir: çµæœç›®éŒ„è·¯å¾‘
        """
        if results_dir is None:
            results_dir = Path(__file__).parent.parent / "test_results" / "mega_test"

        self.results_dir = results_dir
        self.all_results = []
        self.report = {
            'timestamp': datetime.now().isoformat(),
            'total_tests': 0,
            'successful_tests': 0,
            'failed_tests': 0,
            'stages': {},
            'parameters': {},
            'best_params': {},
            'recommendations': []
        }

    def load_results(self) -> bool:
        """
        åŠ è¼‰æ‰€æœ‰æ‰¹æ¬¡çµæœ

        Returns:
            æ˜¯å¦æˆåŠŸåŠ è¼‰
        """
        logger.info("ğŸ“‚ åŠ è¼‰æ¸¬è©¦çµæœ...")

        if not self.results_dir.exists():
            logger.error(f"âŒ çµæœç›®éŒ„ä¸å­˜åœ¨: {self.results_dir}")
            return False

        # æŸ¥æ‰¾æ‰€æœ‰æ‰¹æ¬¡çµæœæ–‡ä»¶
        batch_files = sorted(self.results_dir.glob("batch_*_results.json"))

        if not batch_files:
            logger.error(f"âŒ æ‰¾ä¸åˆ°æ‰¹æ¬¡çµæœæ–‡ä»¶: {self.results_dir}")
            return False

        logger.info(f"æ‰¾åˆ° {len(batch_files)} å€‹æ‰¹æ¬¡çµæœæ–‡ä»¶")

        # åŠ è¼‰æ‰€æœ‰çµæœ
        for batch_file in batch_files:
            try:
                with open(batch_file, 'r', encoding='utf-8') as f:
                    batch_results = json.load(f)
                    self.all_results.extend(batch_results)
                    logger.info(f"  âœ… {batch_file.name}: {len(batch_results)} çµ„")
            except Exception as e:
                logger.error(f"  âŒ {batch_file.name}: {e}")
                continue

        # çµ±è¨ˆ
        self.report['total_tests'] = len(self.all_results)
        self.report['successful_tests'] = sum(1 for r in self.all_results if r.get('success', False))
        self.report['failed_tests'] = self.report['total_tests'] - self.report['successful_tests']

        logger.info(f"\nâœ… æˆåŠŸåŠ è¼‰ {self.report['total_tests']} çµ„æ¸¬è©¦çµæœ")
        logger.info(f"   æˆåŠŸ: {self.report['successful_tests']}")
        logger.info(f"   å¤±æ•—: {self.report['failed_tests']}\n")

        return True

    def analyze_by_stage(self):
        """æŒ‰éšæ®µåˆ†æ"""
        logger.info("ğŸ“Š æŒ‰éšæ®µåˆ†æ...\n")

        # æŒ‰éšæ®µåˆ†çµ„
        stages = defaultdict(list)
        for result in self.all_results:
            if result.get('success', False):
                stage = result.get('stage', 'unknown')
                stages[stage].append(result)

        # åˆ†ææ¯å€‹éšæ®µ
        for stage, results in stages.items():
            logger.info(f"  éšæ®µ: {stage}")
            logger.info(f"    æ¸¬è©¦æ•¸: {len(results)}")

            if results:
                scores = [r['score']['total_score'] for r in results]
                avg_score = sum(scores) / len(scores)
                max_score = max(scores)
                min_score = min(scores)

                logger.info(f"    å¹³å‡åˆ†: {avg_score:.1f}/120")
                logger.info(f"    æœ€é«˜åˆ†: {max_score:.0f}/120")
                logger.info(f"    æœ€ä½åˆ†: {min_score:.0f}/120")

                # æ‰¾å‡ºè©²éšæ®µæœ€ä½³åƒæ•¸
                best_result = max(results, key=lambda r: r['score']['total_score'])
                logger.info(f"    æœ€ä½³åƒæ•¸: {best_result['params']}")

                self.report['stages'][stage] = {
                    'test_count': len(results),
                    'avg_score': avg_score,
                    'max_score': max_score,
                    'min_score': min_score,
                    'best_params': best_result['params'],
                    'best_score': best_result['score']['total_score']
                }

            logger.info("")

    def analyze_by_parameter(self):
        """æŒ‰åƒæ•¸åˆ†æ"""
        logger.info("ğŸ“Š æŒ‰åƒæ•¸åˆ†æ...\n")

        # åªåˆ†ææˆåŠŸçš„çµæœ
        successful_results = [r for r in self.all_results if r.get('success', False)]

        # Temperature åˆ†æ
        self._analyze_single_param('temperature', successful_results)

        # Top_P åˆ†æ
        self._analyze_single_param('top_p', successful_results)

        # Repetition Penalty åˆ†æ
        self._analyze_single_param('repetition_penalty', successful_results)

        # Max Tokens åˆ†æ
        self._analyze_single_param('max_tokens', successful_results)

    def _analyze_single_param(self, param_name: str, results: List[Dict]):
        """
        åˆ†æå–®å€‹åƒæ•¸

        Args:
            param_name: åƒæ•¸åç¨±
            results: æ¸¬è©¦çµæœåˆ—è¡¨
        """
        logger.info(f"  åƒæ•¸: {param_name}")

        # æŒ‰åƒæ•¸å€¼åˆ†çµ„
        param_groups = defaultdict(list)
        for result in results:
            param_value = result['params'].get(param_name)
            if param_value is not None:
                param_groups[param_value].append(result['score']['total_score'])

        if not param_groups:
            logger.info(f"    ç„¡æ•¸æ“š\n")
            return

        # è¨ˆç®—æ¯å€‹å€¼çš„å¹³å‡åˆ†
        param_stats = {}
        for value, scores in param_groups.items():
            avg_score = sum(scores) / len(scores)
            param_stats[value] = {
                'avg_score': avg_score,
                'count': len(scores),
                'min_score': min(scores),
                'max_score': max(scores)
            }

        # æ’åºä¸¦é¡¯ç¤ºå‰ 5 å€‹æœ€ä½³å€¼
        sorted_values = sorted(param_stats.items(), key=lambda x: x[1]['avg_score'], reverse=True)

        logger.info(f"    Top 5 å€¼:")
        for i, (value, stats) in enumerate(sorted_values[:5], 1):
            logger.info(f"      {i}. {value}: {stats['avg_score']:.1f}/120 (n={stats['count']})")

        # ä¿å­˜åˆ°å ±å‘Š
        self.report['parameters'][param_name] = {
            'best_value': sorted_values[0][0],
            'best_avg_score': sorted_values[0][1]['avg_score'],
            'all_values': dict(sorted_values)
        }

        logger.info("")

    def find_best_combinations(self, top_n: int = 10):
        """
        æ‰¾å‡ºæœ€ä½³åƒæ•¸çµ„åˆ

        Args:
            top_n: è¿”å›å‰ N å€‹æœ€ä½³çµ„åˆ
        """
        logger.info(f"ğŸ† å°‹æ‰¾æœ€ä½³åƒæ•¸çµ„åˆ (Top {top_n})...\n")

        # åªåˆ†ææˆåŠŸçš„çµæœ
        successful_results = [r for r in self.all_results if r.get('success', False)]

        if not successful_results:
            logger.warning("  ç„¡æˆåŠŸçš„æ¸¬è©¦çµæœ\n")
            return

        # æŒ‰åˆ†æ•¸æ’åº
        sorted_results = sorted(successful_results, key=lambda r: r['score']['total_score'], reverse=True)

        # é¡¯ç¤ºå‰ N å€‹
        logger.info(f"  æ’å  åˆ†æ•¸    Temperature  Top_P  Penalty  Tokens  éšæ®µ")
        logger.info("  " + "-" * 70)

        best_combinations = []
        for i, result in enumerate(sorted_results[:top_n], 1):
            params = result['params']
            score = result['score']['total_score']
            stage = result.get('stage', 'unknown')

            logger.info(
                f"  #{i:2d}   {score:5.1f}   "
                f"{params['temperature']:5.2f}      "
                f"{params['top_p']:5.2f}  "
                f"{params['repetition_penalty']:5.2f}   "
                f"{params['max_tokens']:5d}   "
                f"{stage}"
            )

            best_combinations.append({
                'rank': i,
                'score': score,
                'params': params,
                'stage': stage,
                'details': result['score']
            })

        self.report['best_params']['top_combinations'] = best_combinations

        logger.info("")

    def analyze_stability(self):
        """åˆ†æç©©å®šæ€§ï¼ˆé‡è¤‡æ¸¬è©¦çš„çµæœï¼‰"""
        logger.info("ğŸ” åˆ†æç©©å®šæ€§...\n")

        # æŸ¥æ‰¾æ‰€æœ‰é‡è¤‡æ¸¬è©¦
        repeated_tests = defaultdict(list)

        for result in self.all_results:
            if result.get('success', False) and 'repeat_id' in result:
                # ä½¿ç”¨åƒæ•¸ä½œç‚ºéµ
                params_key = json.dumps(result['params'], sort_keys=True)
                repeated_tests[params_key].append(result['score']['total_score'])

        if not repeated_tests:
            logger.info("  ç„¡é‡è¤‡æ¸¬è©¦æ•¸æ“š\n")
            return

        # åˆ†ææ¯çµ„é‡è¤‡æ¸¬è©¦
        stability_results = []

        for params_key, scores in repeated_tests.items():
            if len(scores) < 2:
                continue

            import statistics
            avg_score = statistics.mean(scores)
            std_dev = statistics.stdev(scores)
            cv = std_dev / avg_score if avg_score > 0 else 0

            params = json.loads(params_key)

            stability_results.append({
                'params': params,
                'repeat_count': len(scores),
                'avg_score': avg_score,
                'std_dev': std_dev,
                'cv': cv,
                'scores': scores
            })

        # æ’åºï¼ˆæŒ‰æ¨™æº–å·®ï¼‰
        stability_results.sort(key=lambda x: x['std_dev'])

        # é¡¯ç¤ºæœ€ç©©å®šçš„çµ„åˆ
        logger.info("  æœ€ç©©å®šçš„åƒæ•¸çµ„åˆ (æ¨™æº–å·®æœ€å°):")
        logger.info(f"  æ’å  å¹³å‡åˆ†  æ¨™æº–å·®  CV     é‡è¤‡æ¬¡æ•¸")
        logger.info("  " + "-" * 50)

        for i, result in enumerate(stability_results[:5], 1):
            logger.info(
                f"  #{i}   {result['avg_score']:6.1f}  "
                f"{result['std_dev']:5.2f}   "
                f"{result['cv']:5.2%}  "
                f"{result['repeat_count']:3d}"
            )

        self.report['stability'] = stability_results

        logger.info("")

    def generate_recommendations(self):
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        logger.info("ğŸ’¡ ç”Ÿæˆå„ªåŒ–å»ºè­°...\n")

        recommendations = []

        # å»ºè­° 1: æœ€ä½³å–®åƒæ•¸å€¼
        if 'parameters' in self.report:
            for param_name, param_data in self.report['parameters'].items():
                best_value = param_data['best_value']
                best_score = param_data['best_avg_score']

                recommendations.append({
                    'type': 'best_single_param',
                    'param': param_name,
                    'value': best_value,
                    'avg_score': best_score,
                    'description': f'æœ€ä½³ {param_name} å€¼ç‚º {best_value}ï¼ˆå¹³å‡åˆ† {best_score:.1f}ï¼‰'
                })

        # å»ºè­° 2: æœ€ä½³çµ„åˆ
        if 'best_params' in self.report and 'top_combinations' in self.report['best_params']:
            top_combo = self.report['best_params']['top_combinations'][0]

            recommendations.append({
                'type': 'best_combination',
                'params': top_combo['params'],
                'score': top_combo['score'],
                'description': f'æœ€ä½³åƒæ•¸çµ„åˆå¾—åˆ† {top_combo["score"]:.1f}/120'
            })

        # å»ºè­° 3: ç©©å®šæ€§å»ºè­°
        if 'stability' in self.report and self.report['stability']:
            most_stable = self.report['stability'][0]

            recommendations.append({
                'type': 'most_stable',
                'params': most_stable['params'],
                'std_dev': most_stable['std_dev'],
                'avg_score': most_stable['avg_score'],
                'description': f'æœ€ç©©å®šçš„åƒæ•¸çµ„åˆï¼ˆæ¨™æº–å·® {most_stable["std_dev"]:.2f}ï¼‰'
            })

        # é¡¯ç¤ºå»ºè­°
        for i, rec in enumerate(recommendations, 1):
            logger.info(f"  {i}. {rec['description']}")
            if 'params' in rec:
                logger.info(f"     åƒæ•¸: {rec['params']}")

        self.report['recommendations'] = recommendations

        logger.info("")

    def save_report(self):
        """ä¿å­˜åˆ†æå ±å‘Š"""
        # ä¿å­˜ JSON å ±å‘Š
        report_file = self.results_dir / f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.report, f, ensure_ascii=False, indent=2)

        # ç”Ÿæˆ Markdown å ±å‘Š
        md_file = self.results_dir / f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"

        with open(md_file, 'w', encoding='utf-8') as f:
            f.write("# GLM-4 è¶…å¤§è¦æ¨¡åƒæ•¸æ¸¬è©¦åˆ†æå ±å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ™‚é–“**: {self.report['timestamp']}\n\n")

            # ç¸½é«”çµ±è¨ˆ
            f.write("## ğŸ“Š ç¸½é«”çµ±è¨ˆ\n\n")
            f.write(f"- ç¸½æ¸¬è©¦æ•¸: {self.report['total_tests']}\n")
            f.write(f"- æˆåŠŸ: {self.report['successful_tests']} ({self.report['successful_tests']/self.report['total_tests']*100:.1f}%)\n")
            f.write(f"- å¤±æ•—: {self.report['failed_tests']} ({self.report['failed_tests']/self.report['total_tests']*100:.1f}%)\n\n")

            # éšæ®µåˆ†æ
            if 'stages' in self.report:
                f.write("## ğŸ“ˆ éšæ®µåˆ†æ\n\n")

                for stage, data in self.report['stages'].items():
                    f.write(f"### {stage.upper()}\n\n")
                    f.write(f"- æ¸¬è©¦æ•¸: {data['test_count']}\n")
                    f.write(f"- å¹³å‡åˆ†: {data['avg_score']:.1f}/120\n")
                    f.write(f"- åˆ†æ•¸ç¯„åœ: {data['min_score']:.0f} - {data['max_score']:.0f}\n")
                    f.write(f"- æœ€ä½³åƒæ•¸: `{data['best_params']}`\n")
                    f.write(f"- æœ€ä½³åˆ†æ•¸: {data['best_score']:.0f}/120\n\n")

            # åƒæ•¸åˆ†æ
            if 'parameters' in self.report:
                f.write("## ğŸ”§ åƒæ•¸åˆ†æ\n\n")

                for param_name, param_data in self.report['parameters'].items():
                    f.write(f"### {param_name}\n\n")
                    f.write(f"- æœ€ä½³å€¼: {param_data['best_value']}\n")
                    f.write(f"- æœ€ä½³å¹³å‡åˆ†: {param_data['best_avg_score']:.1f}/120\n\n")

            # æœ€ä½³çµ„åˆ
            if 'best_params' in self.report and 'top_combinations' in self.report['best_params']:
                f.write("## ğŸ† æœ€ä½³åƒæ•¸çµ„åˆ (Top 10)\n\n")
                f.write("| æ’å | åˆ†æ•¸ | Temperature | Top_P | Penalty | Tokens | éšæ®µ |\n")
                f.write("|------|------|-------------|-------|---------|--------|------|\n")

                for combo in self.report['best_params']['top_combinations']:
                    params = combo['params']
                    f.write(
                        f"| #{combo['rank']} | {combo['score']:.1f} | "
                        f"{params['temperature']:.2f} | "
                        f"{params['top_p']:.2f} | "
                        f"{params['repetition_penalty']:.2f} | "
                        f"{params['max_tokens']} | "
                        f"{combo['stage']} |\n"
                    )

                f.write("\n")

            # å„ªåŒ–å»ºè­°
            if 'recommendations' in self.report:
                f.write("## ğŸ’¡ å„ªåŒ–å»ºè­°\n\n")

                for i, rec in enumerate(self.report['recommendations'], 1):
                    f.write(f"{i}. **{rec['description']}**\n")
                    if 'params' in rec:
                        f.write(f"   ```json\n   {json.dumps(rec['params'], indent=2, ensure_ascii=False)}\n   ```\n")
                    f.write("\n")

        logger.info(f"âœ… å ±å‘Šå·²ä¿å­˜:")
        logger.info(f"   JSON: {report_file}")
        logger.info(f"   Markdown: {md_file}\n")

    def run_analysis(self):
        """åŸ·è¡Œå®Œæ•´åˆ†æ"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ”¬ é–‹å§‹åˆ†æè¶…å¤§è¦æ¨¡æ¸¬è©¦çµæœ")
        logger.info("="*60 + "\n")

        # åŠ è¼‰çµæœ
        if not self.load_results():
            return

        # åŸ·è¡Œå„é …åˆ†æ
        self.analyze_by_stage()
        self.analyze_by_parameter()
        self.find_best_combinations(top_n=10)
        self.analyze_stability()
        self.generate_recommendations()

        # ä¿å­˜å ±å‘Š
        self.save_report()

        logger.info("="*60)
        logger.info("âœ… åˆ†æå®Œæˆ")
        logger.info("="*60 + "\n")


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='åˆ†æè¶…å¤§è¦æ¨¡æ¸¬è©¦çµæœ')

    parser.add_argument('--stage', choices=['coarse', 'fine', 'validation'],
                        help='åªåˆ†ææŒ‡å®šéšæ®µ')
    parser.add_argument('--visualize', action='store_true',
                        help='ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨ï¼ˆéœ€è¦ matplotlibï¼‰')
    parser.add_argument('--results-dir', type=Path,
                        help='æŒ‡å®šçµæœç›®éŒ„')

    args = parser.parse_args()

    # å‰µå»ºåˆ†æå™¨
    analyzer = MegaResultsAnalyzer(results_dir=args.results_dir)

    # åŸ·è¡Œåˆ†æ
    analyzer.run_analysis()


if __name__ == "__main__":
    main()
