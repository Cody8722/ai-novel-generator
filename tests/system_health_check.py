#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å…¨é¢ç³»çµ±å¥åº·æª¢æŸ¥

æª¢æŸ¥é …ç›®ï¼š
1. GLM-4 ç‰¹æœ‰æŒ‡æ¨™åŠŸèƒ½
2. åƒæ•¸å½±éŸ¿åˆ†æ
3. è©•åˆ†ç³»çµ±ä¸€è‡´æ€§
4. API ç©©å®šæ€§
5. è¼¸å‡ºå“è³ªé©—è­‰

ä½¿ç”¨æ–¹æ³•ï¼š
    # åŸ·è¡Œå®Œæ•´æª¢æŸ¥
    python tests/system_health_check.py

    # åŸ·è¡Œç‰¹å®šæª¢æŸ¥
    python tests/system_health_check.py --check glm4-metrics
    python tests/system_health_check.py --check param-impact
    python tests/system_health_check.py --check scoring-consistency
    python tests/system_health_check.py --check api-stability
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import json
import time
import logging
import statistics
import argparse
from datetime import datetime
from typing import Dict, List, Tuple
from tests.test_glm4_params import GLM4ParamsTester

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SystemHealthChecker:
    """ç³»çµ±å¥åº·æª¢æŸ¥å™¨"""

    def __init__(self, api_key: str):
        """
        åˆå§‹åŒ–ç³»çµ±æª¢æŸ¥å™¨

        Args:
            api_key: API é‡‘é‘°
        """
        self.api_key = api_key
        self.output_dir = Path(__file__).parent.parent / "test_results" / "health_check"
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.report = {
            'timestamp': datetime.now().isoformat(),
            'checks': {},
            'overall_status': 'unknown'
        }

    def check_glm4_metrics(self) -> Dict:
        """
        æª¢æŸ¥ GLM-4 ç‰¹æœ‰æŒ‡æ¨™

        Returns:
            æª¢æŸ¥çµæœ
        """
        logger.info("\n" + "="*60)
        logger.info("ğŸ” æª¢æŸ¥ GLM-4 ç‰¹æœ‰æŒ‡æ¨™")
        logger.info("="*60 + "\n")

        results = {
            'status': 'unknown',
            'test_count': 10,
            'metrics': {},
            'issues': []
        }

        # å‰µå»ºæ¸¬è©¦å™¨
        tester = GLM4ParamsTester(
            self.api_key,
            quick_mode=False,
            enable_ai_review=False,
            debug_mode=False
        )

        # æ¸¬è©¦åƒæ•¸
        test_params = {
            'temperature': 0.7,
            'top_p': 0.9,
            'repetition_penalty': 1.05,
            'max_tokens': 4000
        }

        # ç”Ÿæˆæ¸¬è©¦å¤§ç¶±ä¸¦è©•ä¼°
        logger.info("ğŸ“ ç”Ÿæˆ 10 å€‹æ¸¬è©¦å¤§ç¶±...")
        metrics_data = {
            'chinese_fluency': [],
            'cultural_depth': [],
            'creativity': [],
            'coherence': [],
            'glm4_score': []
        }

        for i in range(10):
            logger.info(f"  æ¸¬è©¦ {i+1}/10...")

            try:
                outline, cost, score = tester.test_params(test_params)

                # æå– GLM-4 æŒ‡æ¨™
                glm4_checks = score.get('glm4_checks', {})

                if glm4_checks:
                    metrics_data['chinese_fluency'].append(glm4_checks.get('chinese_fluency', 0))
                    metrics_data['cultural_depth'].append(glm4_checks.get('cultural_depth', 0))
                    metrics_data['creativity'].append(glm4_checks.get('creativity', 0))
                    metrics_data['coherence'].append(glm4_checks.get('coherence', 0))
                    metrics_data['glm4_score'].append(score.get('glm4_score', 0))
                else:
                    logger.warning(f"    âš ï¸ æ¸¬è©¦ {i+1} æ²’æœ‰ GLM-4 æŒ‡æ¨™æ•¸æ“š")

                # API å»¶é²
                time.sleep(3)

            except Exception as e:
                logger.error(f"    âŒ æ¸¬è©¦ {i+1} å¤±æ•—: {e}")
                continue

        # åˆ†æçµæœ
        logger.info("\nğŸ“Š åˆ†ææŒ‡æ¨™æ•¸æ“š...\n")

        for metric, values in metrics_data.items():
            if not values:
                logger.warning(f"  âš ï¸ {metric}: ç„¡æ•¸æ“š")
                results['issues'].append(f"{metric} has no data")
                continue

            avg = sum(values) / len(values)
            non_zero = sum(1 for v in values if v > 0)
            zero_rate = (len(values) - non_zero) / len(values) * 100

            logger.info(f"  {metric}:")
            logger.info(f"    æ•¸æ“šé»: {len(values)}")
            logger.info(f"    å¹³å‡å€¼: {avg:.2f}")
            logger.info(f"    æœ€å°å€¼: {min(values):.2f}")
            logger.info(f"    æœ€å¤§å€¼: {max(values):.2f}")
            logger.info(f"    éé›¶æ¯”ä¾‹: {non_zero}/{len(values)} ({non_zero/len(values)*100:.1f}%)")

            results['metrics'][metric] = {
                'avg': avg,
                'min': min(values),
                'max': max(values),
                'non_zero_rate': non_zero / len(values),
                'zero_rate': zero_rate
            }

            # æª¢æŸ¥å•é¡Œ
            if zero_rate > 50:
                logger.warning(f"    âš ï¸ {metric} é›¶å€¼æ¯”ä¾‹éé«˜ ({zero_rate:.1f}%)")
                results['issues'].append(f"{metric} has high zero rate: {zero_rate:.1f}%")
            elif zero_rate > 20:
                logger.warning(f"    âš ï¸ {metric} é›¶å€¼æ¯”ä¾‹åé«˜ ({zero_rate:.1f}%)")
                results['issues'].append(f"{metric} has elevated zero rate: {zero_rate:.1f}%")
            else:
                logger.info(f"    âœ… {metric} è¡¨ç¾æ­£å¸¸")

        # åˆ¤æ–·ç‹€æ…‹
        if not results['issues']:
            results['status'] = 'healthy'
            logger.info("\nâœ… GLM-4 æŒ‡æ¨™æª¢æŸ¥é€šé\n")
        elif len(results['issues']) <= 2:
            results['status'] = 'warning'
            logger.warning(f"\nâš ï¸ GLM-4 æŒ‡æ¨™æª¢æŸ¥ç™¼ç¾ {len(results['issues'])} å€‹è­¦å‘Š\n")
        else:
            results['status'] = 'critical'
            logger.error(f"\nâŒ GLM-4 æŒ‡æ¨™æª¢æŸ¥å¤±æ•—ï¼š{len(results['issues'])} å€‹å•é¡Œ\n")

        return results

    def check_param_impact(self) -> Dict:
        """
        æª¢æŸ¥åƒæ•¸å½±éŸ¿

        Returns:
            æª¢æŸ¥çµæœ
        """
        logger.info("\n" + "="*60)
        logger.info("ğŸ” æª¢æŸ¥åƒæ•¸ç¨ç«‹å½±éŸ¿")
        logger.info("="*60 + "\n")

        results = {
            'status': 'unknown',
            'parameters': {},
            'issues': []
        }

        # å‰µå»ºæ¸¬è©¦å™¨
        tester = GLM4ParamsTester(
            self.api_key,
            quick_mode=False,
            enable_ai_review=False,
            debug_mode=False
        )

        # æ¸¬è©¦ Temperature å½±éŸ¿
        logger.info("ğŸ“ˆ æ¸¬è©¦ Temperature å½±éŸ¿...")
        temp_data = []
        for temp in [0.5, 0.6, 0.7, 0.8, 0.9]:
            logger.info(f"  Temperature = {temp}")
            params = {
                'temperature': temp,
                'top_p': 0.9,
                'repetition_penalty': 1.05,
                'max_tokens': 4000
            }

            try:
                outline, cost, score = tester.test_params(params)
                temp_data.append((temp, score['total_score']))
                logger.info(f"    åˆ†æ•¸: {score['total_score']:.0f}/120")
                time.sleep(3)
            except Exception as e:
                logger.error(f"    âŒ å¤±æ•—: {e}")
                continue

        # åˆ†æ Temperature è¶¨å‹¢
        temp_trend = self._analyze_trend('Temperature', temp_data)
        results['parameters']['temperature'] = temp_trend

        # æ¸¬è©¦ Top_P å½±éŸ¿
        logger.info("\nğŸ“ˆ æ¸¬è©¦ Top_P å½±éŸ¿...")
        topp_data = []
        for top_p in [0.8, 0.85, 0.9, 0.95]:
            logger.info(f"  Top_P = {top_p}")
            params = {
                'temperature': 0.7,
                'top_p': top_p,
                'repetition_penalty': 1.05,
                'max_tokens': 4000
            }

            try:
                outline, cost, score = tester.test_params(params)
                topp_data.append((top_p, score['total_score']))
                logger.info(f"    åˆ†æ•¸: {score['total_score']:.0f}/120")
                time.sleep(3)
            except Exception as e:
                logger.error(f"    âŒ å¤±æ•—: {e}")
                continue

        topp_trend = self._analyze_trend('Top_P', topp_data)
        results['parameters']['top_p'] = topp_trend

        # æ¸¬è©¦ Repetition Penalty å½±éŸ¿
        logger.info("\nğŸ“ˆ æ¸¬è©¦ Repetition Penalty å½±éŸ¿...")
        penalty_data = []
        for penalty in [1.0, 1.05, 1.1, 1.15]:
            logger.info(f"  Penalty = {penalty}")
            params = {
                'temperature': 0.7,
                'top_p': 0.9,
                'repetition_penalty': penalty,
                'max_tokens': 4000
            }

            try:
                outline, cost, score = tester.test_params(params)
                penalty_data.append((penalty, score['total_score']))
                logger.info(f"    åˆ†æ•¸: {score['total_score']:.0f}/120")
                time.sleep(3)
            except Exception as e:
                logger.error(f"    âŒ å¤±æ•—: {e}")
                continue

        penalty_trend = self._analyze_trend('Repetition Penalty', penalty_data)
        results['parameters']['repetition_penalty'] = penalty_trend

        # åˆ¤æ–·ç‹€æ…‹
        if not results['issues']:
            results['status'] = 'healthy'
            logger.info("\nâœ… åƒæ•¸å½±éŸ¿æª¢æŸ¥é€šé\n")
        else:
            results['status'] = 'warning'
            logger.warning(f"\nâš ï¸ åƒæ•¸å½±éŸ¿æª¢æŸ¥ç™¼ç¾ {len(results['issues'])} å€‹ç•°å¸¸\n")

        return results

    def _analyze_trend(self, param_name: str, data: List[Tuple[float, float]]) -> Dict:
        """
        åˆ†æåƒæ•¸è¶¨å‹¢

        Args:
            param_name: åƒæ•¸åç¨±
            data: (åƒæ•¸å€¼, åˆ†æ•¸) åˆ—è¡¨

        Returns:
            è¶¨å‹¢åˆ†æçµæœ
        """
        if len(data) < 2:
            return {
                'trend': 'insufficient_data',
                'data': data,
                'description': 'æ•¸æ“šä¸è¶³'
            }

        # è¨ˆç®—è¶¨å‹¢
        values = [d[0] for d in data]
        scores = [d[1] for d in data]

        # ç°¡å–®ç·šæ€§è¶¨å‹¢æª¢æ¸¬
        increasing = sum(1 for i in range(len(scores) - 1) if scores[i + 1] > scores[i])
        decreasing = sum(1 for i in range(len(scores) - 1) if scores[i + 1] < scores[i])

        if increasing > decreasing:
            trend = 'increasing'
            description = f'{param_name} å¢åŠ æ™‚åˆ†æ•¸ä¸Šå‡'
        elif decreasing > increasing:
            trend = 'decreasing'
            description = f'{param_name} å¢åŠ æ™‚åˆ†æ•¸ä¸‹é™'
        else:
            trend = 'stable'
            description = f'{param_name} å°åˆ†æ•¸å½±éŸ¿ä¸æ˜é¡¯'

        # æ‰¾å‡ºæœ€ä½³å€¼
        best_idx = scores.index(max(scores))
        best_value = values[best_idx]
        best_score = scores[best_idx]

        logger.info(f"\n  {param_name} è¶¨å‹¢åˆ†æ:")
        logger.info(f"    è¶¨å‹¢: {description}")
        logger.info(f"    æœ€ä½³å€¼: {best_value} (åˆ†æ•¸: {best_score:.0f})")
        logger.info(f"    åˆ†æ•¸ç¯„åœ: {min(scores):.0f} - {max(scores):.0f}")

        return {
            'trend': trend,
            'description': description,
            'best_value': best_value,
            'best_score': best_score,
            'score_range': (min(scores), max(scores)),
            'data': data
        }

    def check_scoring_consistency(self) -> Dict:
        """
        æª¢æŸ¥è©•åˆ†ä¸€è‡´æ€§

        Returns:
            æª¢æŸ¥çµæœ
        """
        logger.info("\n" + "="*60)
        logger.info("ğŸ” æª¢æŸ¥è©•åˆ†ä¸€è‡´æ€§")
        logger.info("="*60 + "\n")

        results = {
            'status': 'unknown',
            'repetitions': 5,
            'scores': [],
            'statistics': {},
            'issues': []
        }

        # å‰µå»ºæ¸¬è©¦å™¨
        tester = GLM4ParamsTester(
            self.api_key,
            quick_mode=False,
            enable_ai_review=False,
            debug_mode=False
        )

        # å›ºå®šåƒæ•¸
        params = {
            'temperature': 0.7,
            'top_p': 0.9,
            'repetition_penalty': 1.05,
            'max_tokens': 4000
        }

        logger.info(f"ğŸ“ ä½¿ç”¨ç›¸åŒåƒæ•¸ç”Ÿæˆ {results['repetitions']} æ¬¡...")
        logger.info(f"   åƒæ•¸: {params}\n")

        scores = []
        for i in range(results['repetitions']):
            logger.info(f"  é‡è¤‡æ¸¬è©¦ {i+1}/{results['repetitions']}")

            try:
                outline, cost, score = tester.test_params(params)
                scores.append(score['total_score'])
                logger.info(f"    åˆ†æ•¸: {score['total_score']:.0f}/120")
                time.sleep(3)
            except Exception as e:
                logger.error(f"    âŒ å¤±æ•—: {e}")
                continue

        if len(scores) < 2:
            results['status'] = 'critical'
            results['issues'].append('æ¸¬è©¦æ•¸æ“šä¸è¶³')
            logger.error("\nâŒ è©•åˆ†ä¸€è‡´æ€§æª¢æŸ¥å¤±æ•—ï¼šæ•¸æ“šä¸è¶³\n")
            return results

        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        avg_score = statistics.mean(scores)
        std_dev = statistics.stdev(scores)
        min_score = min(scores)
        max_score = max(scores)
        score_range = max_score - min_score

        results['scores'] = scores
        results['statistics'] = {
            'mean': avg_score,
            'std_dev': std_dev,
            'min': min_score,
            'max': max_score,
            'range': score_range,
            'cv': std_dev / avg_score if avg_score > 0 else 0  # è®Šç•°ä¿‚æ•¸
        }

        logger.info(f"\nğŸ“Š çµ±è¨ˆåˆ†æ:")
        logger.info(f"  åˆ†æ•¸åˆ—è¡¨: {[f'{s:.0f}' for s in scores]}")
        logger.info(f"  å¹³å‡åˆ†: {avg_score:.1f}")
        logger.info(f"  æ¨™æº–å·®: {std_dev:.2f}")
        logger.info(f"  æœ€å°å€¼: {min_score:.0f}")
        logger.info(f"  æœ€å¤§å€¼: {max_score:.0f}")
        logger.info(f"  åˆ†æ•¸ç¯„åœ: {score_range:.1f}")
        logger.info(f"  è®Šç•°ä¿‚æ•¸: {std_dev / avg_score:.2%}")

        # åˆ¤æ–·ä¸€è‡´æ€§
        if std_dev > 15:
            results['status'] = 'critical'
            results['issues'].append(f'æ¨™æº–å·®éå¤§ ({std_dev:.2f})')
            logger.error(f"\nâŒ è©•åˆ†æ³¢å‹•éå¤§ï¼šæ¨™æº–å·® {std_dev:.2f} > 15\n")
        elif std_dev > 10:
            results['status'] = 'warning'
            results['issues'].append(f'æ¨™æº–å·®åé«˜ ({std_dev:.2f})')
            logger.warning(f"\nâš ï¸ è©•åˆ†æ³¢å‹•åé«˜ï¼šæ¨™æº–å·® {std_dev:.2f} > 10\n")
        else:
            results['status'] = 'healthy'
            logger.info(f"\nâœ… è©•åˆ†ä¸€è‡´æ€§è‰¯å¥½ï¼šæ¨™æº–å·® {std_dev:.2f} <= 10\n")

        return results

    def check_api_stability(self) -> Dict:
        """
        æª¢æŸ¥ API ç©©å®šæ€§

        Returns:
            æª¢æŸ¥çµæœ
        """
        logger.info("\n" + "="*60)
        logger.info("ğŸ” æª¢æŸ¥ API ç©©å®šæ€§")
        logger.info("="*60 + "\n")

        results = {
            'status': 'unknown',
            'test_count': 10,
            'success_count': 0,
            'failure_count': 0,
            'response_times': [],
            'issues': []
        }

        # å‰µå»ºæ¸¬è©¦å™¨
        tester = GLM4ParamsTester(
            self.api_key,
            quick_mode=False,
            enable_ai_review=False,
            debug_mode=False
        )

        params = {
            'temperature': 0.7,
            'top_p': 0.9,
            'repetition_penalty': 1.05,
            'max_tokens': 4000
        }

        logger.info(f"ğŸ”„ åŸ·è¡Œ {results['test_count']} æ¬¡ API è«‹æ±‚...\n")

        for i in range(results['test_count']):
            logger.info(f"  è«‹æ±‚ {i+1}/{results['test_count']}")

            start_time = time.time()
            try:
                outline, cost, score = tester.test_params(params)
                response_time = time.time() - start_time

                results['success_count'] += 1
                results['response_times'].append(response_time)

                logger.info(f"    âœ… æˆåŠŸ ({response_time:.1f}ç§’)")
                time.sleep(2)

            except Exception as e:
                response_time = time.time() - start_time
                results['failure_count'] += 1

                logger.error(f"    âŒ å¤±æ•—: {e}")
                continue

        # åˆ†æçµæœ
        success_rate = results['success_count'] / results['test_count']

        if results['response_times']:
            avg_response_time = statistics.mean(results['response_times'])
            max_response_time = max(results['response_times'])
            min_response_time = min(results['response_times'])
        else:
            avg_response_time = max_response_time = min_response_time = 0

        results['success_rate'] = success_rate
        results['avg_response_time'] = avg_response_time
        results['max_response_time'] = max_response_time
        results['min_response_time'] = min_response_time

        logger.info(f"\nğŸ“Š ç©©å®šæ€§åˆ†æ:")
        logger.info(f"  æˆåŠŸ: {results['success_count']}/{results['test_count']} ({success_rate:.1%})")
        logger.info(f"  å¤±æ•—: {results['failure_count']}/{results['test_count']}")
        logger.info(f"  å¹³å‡éŸ¿æ‡‰æ™‚é–“: {avg_response_time:.1f}ç§’")
        logger.info(f"  æœ€å¿«éŸ¿æ‡‰: {min_response_time:.1f}ç§’")
        logger.info(f"  æœ€æ…¢éŸ¿æ‡‰: {max_response_time:.1f}ç§’")

        # åˆ¤æ–·ç‹€æ…‹
        if success_rate < 0.8:
            results['status'] = 'critical'
            results['issues'].append(f'æˆåŠŸç‡éä½ ({success_rate:.1%})')
            logger.error(f"\nâŒ API ä¸ç©©å®šï¼šæˆåŠŸç‡ {success_rate:.1%} < 80%\n")
        elif success_rate < 0.95:
            results['status'] = 'warning'
            results['issues'].append(f'æˆåŠŸç‡åä½ ({success_rate:.1%})')
            logger.warning(f"\nâš ï¸ API ç©©å®šæ€§ä¸€èˆ¬ï¼šæˆåŠŸç‡ {success_rate:.1%} < 95%\n")
        else:
            results['status'] = 'healthy'
            logger.info(f"\nâœ… API ç©©å®šæ€§è‰¯å¥½ï¼šæˆåŠŸç‡ {success_rate:.1%}\n")

        return results

    def run_all_checks(self):
        """åŸ·è¡Œæ‰€æœ‰æª¢æŸ¥"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ¥ é–‹å§‹ç³»çµ±å¥åº·æª¢æŸ¥")
        logger.info("="*60 + "\n")

        start_time = time.time()

        # åŸ·è¡Œå„é …æª¢æŸ¥
        self.report['checks']['glm4_metrics'] = self.check_glm4_metrics()
        self.report['checks']['param_impact'] = self.check_param_impact()
        self.report['checks']['scoring_consistency'] = self.check_scoring_consistency()
        self.report['checks']['api_stability'] = self.check_api_stability()

        # è¨ˆç®—æ•´é«”ç‹€æ…‹
        statuses = [check['status'] for check in self.report['checks'].values()]

        if 'critical' in statuses:
            self.report['overall_status'] = 'critical'
        elif 'warning' in statuses:
            self.report['overall_status'] = 'warning'
        else:
            self.report['overall_status'] = 'healthy'

        # ä¿å­˜å ±å‘Š
        total_time = time.time() - start_time
        self.report['total_time'] = total_time

        report_file = self.output_dir / f"health_check_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.report, f, ensure_ascii=False, indent=2)

        # ç”Ÿæˆæ‘˜è¦
        self._print_summary(total_time, report_file)

    def _print_summary(self, total_time: float, report_file: Path):
        """åˆ—å°æ‘˜è¦"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“‹ å¥åº·æª¢æŸ¥æ‘˜è¦")
        logger.info("="*60)

        for check_name, check_result in self.report['checks'].items():
            status = check_result['status']
            status_emoji = {
                'healthy': 'âœ…',
                'warning': 'âš ï¸',
                'critical': 'âŒ',
                'unknown': 'â“'
            }.get(status, 'â“')

            logger.info(f"{status_emoji} {check_name}: {status.upper()}")

            if check_result.get('issues'):
                for issue in check_result['issues']:
                    logger.info(f"    - {issue}")

        logger.info(f"\næ•´é«”ç‹€æ…‹: {self.report['overall_status'].upper()}")
        logger.info(f"ç¸½è€—æ™‚: {total_time:.1f}ç§’")
        logger.info(f"å ±å‘Šå·²ä¿å­˜: {report_file}")
        logger.info("="*60 + "\n")


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='ç³»çµ±å¥åº·æª¢æŸ¥')
    parser.add_argument(
        '--check',
        choices=['glm4-metrics', 'param-impact', 'scoring-consistency', 'api-stability', 'all'],
        default='all',
        help='æŒ‡å®šæª¢æŸ¥é …ç›®ï¼ˆé»˜èª: allï¼‰'
    )

    args = parser.parse_args()

    # ç²å– API Key
    import os
    from dotenv import load_dotenv
    load_dotenv()
    api_key = os.getenv("SILICONFLOW_API_KEY")

    if not api_key:
        logger.error("âŒ æ‰¾ä¸åˆ° SILICONFLOW_API_KEY ç’°å¢ƒè®Šé‡")
        return

    # å‰µå»ºæª¢æŸ¥å™¨
    checker = SystemHealthChecker(api_key)

    # åŸ·è¡Œæª¢æŸ¥
    if args.check == 'all':
        checker.run_all_checks()
    elif args.check == 'glm4-metrics':
        result = checker.check_glm4_metrics()
        checker.report['checks']['glm4_metrics'] = result
    elif args.check == 'param-impact':
        result = checker.check_param_impact()
        checker.report['checks']['param_impact'] = result
    elif args.check == 'scoring-consistency':
        result = checker.check_scoring_consistency()
        checker.report['checks']['scoring_consistency'] = result
    elif args.check == 'api-stability':
        result = checker.check_api_stability()
        checker.report['checks']['api_stability'] = result


if __name__ == "__main__":
    main()
